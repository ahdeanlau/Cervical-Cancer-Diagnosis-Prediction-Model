{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "- Use this template to develop your project. Do not change the steps. \n",
    "- For each step, you may add additional cells if needed.\n",
    "- Describe the steps in the \"Description:\" field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Information\n",
    "\n",
    "Group No: Cancer_2\n",
    "\n",
    "- Member 1: Foo Ling Zhen\n",
    "- Member 2: Dean Lau Sheng Ting\n",
    "- Member 3: Looi Wei En\n",
    "- Member 4: Eason Peng \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of sexual partners</th>\n",
       "      <th>First sexual intercourse</th>\n",
       "      <th>Num of pregnancies</th>\n",
       "      <th>Smokes</th>\n",
       "      <th>Smokes (years)</th>\n",
       "      <th>Smokes (packs/year)</th>\n",
       "      <th>Hormonal Contraceptives</th>\n",
       "      <th>Hormonal Contraceptives (years)</th>\n",
       "      <th>IUD</th>\n",
       "      <th>...</th>\n",
       "      <th>STDs: Time since first diagnosis</th>\n",
       "      <th>STDs: Time since last diagnosis</th>\n",
       "      <th>Dx:Cancer</th>\n",
       "      <th>Dx:CIN</th>\n",
       "      <th>Dx:HPV</th>\n",
       "      <th>Dx</th>\n",
       "      <th>Hinselmann</th>\n",
       "      <th>Schiller</th>\n",
       "      <th>Citology</th>\n",
       "      <th>Biopsy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>34</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>33</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>858 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Number of sexual partners First sexual intercourse  \\\n",
       "0     18                       4.0                     15.0   \n",
       "1     15                       1.0                     14.0   \n",
       "2     34                       1.0                        ?   \n",
       "3     52                       5.0                     16.0   \n",
       "4     46                       3.0                     21.0   \n",
       "..   ...                       ...                      ...   \n",
       "853   34                       3.0                     18.0   \n",
       "854   32                       2.0                     19.0   \n",
       "855   25                       2.0                     17.0   \n",
       "856   33                       2.0                     24.0   \n",
       "857   29                       2.0                     20.0   \n",
       "\n",
       "    Num of pregnancies Smokes Smokes (years) Smokes (packs/year)  \\\n",
       "0                  1.0    0.0            0.0                 0.0   \n",
       "1                  1.0    0.0            0.0                 0.0   \n",
       "2                  1.0    0.0            0.0                 0.0   \n",
       "3                  4.0    1.0           37.0                37.0   \n",
       "4                  4.0    0.0            0.0                 0.0   \n",
       "..                 ...    ...            ...                 ...   \n",
       "853                0.0    0.0            0.0                 0.0   \n",
       "854                1.0    0.0            0.0                 0.0   \n",
       "855                0.0    0.0            0.0                 0.0   \n",
       "856                2.0    0.0            0.0                 0.0   \n",
       "857                1.0    0.0            0.0                 0.0   \n",
       "\n",
       "    Hormonal Contraceptives Hormonal Contraceptives (years)  IUD  ...  \\\n",
       "0                       0.0                             0.0  0.0  ...   \n",
       "1                       0.0                             0.0  0.0  ...   \n",
       "2                       0.0                             0.0  0.0  ...   \n",
       "3                       1.0                             3.0  0.0  ...   \n",
       "4                       1.0                            15.0  0.0  ...   \n",
       "..                      ...                             ...  ...  ...   \n",
       "853                     0.0                             0.0  0.0  ...   \n",
       "854                     1.0                             8.0  0.0  ...   \n",
       "855                     1.0                            0.08  0.0  ...   \n",
       "856                     1.0                            0.08  0.0  ...   \n",
       "857                     1.0                             0.5  0.0  ...   \n",
       "\n",
       "    STDs: Time since first diagnosis STDs: Time since last diagnosis  \\\n",
       "0                                  ?                               ?   \n",
       "1                                  ?                               ?   \n",
       "2                                  ?                               ?   \n",
       "3                                  ?                               ?   \n",
       "4                                  ?                               ?   \n",
       "..                               ...                             ...   \n",
       "853                                ?                               ?   \n",
       "854                                ?                               ?   \n",
       "855                                ?                               ?   \n",
       "856                                ?                               ?   \n",
       "857                                ?                               ?   \n",
       "\n",
       "    Dx:Cancer Dx:CIN Dx:HPV Dx Hinselmann Schiller Citology Biopsy  \n",
       "0           0      0      0  0          0        0        0      0  \n",
       "1           0      0      0  0          0        0        0      0  \n",
       "2           0      0      0  0          0        0        0      0  \n",
       "3           1      0      1  0          0        0        0      0  \n",
       "4           0      0      0  0          0        0        0      0  \n",
       "..        ...    ...    ... ..        ...      ...      ...    ...  \n",
       "853         0      0      0  0          0        0        0      0  \n",
       "854         0      0      0  0          0        0        0      0  \n",
       "855         0      0      0  0          0        0        1      0  \n",
       "856         0      0      0  0          0        0        0      0  \n",
       "857         0      0      0  0          0        0        0      0  \n",
       "\n",
       "[858 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('risk_factors.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(858, 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 858 entries, 0 to 857\n",
      "Data columns (total 36 columns):\n",
      " #   Column                              Non-Null Count  Dtype \n",
      "---  ------                              --------------  ----- \n",
      " 0   Age                                 858 non-null    int64 \n",
      " 1   Number of sexual partners           858 non-null    object\n",
      " 2   First sexual intercourse            858 non-null    object\n",
      " 3   Num of pregnancies                  858 non-null    object\n",
      " 4   Smokes                              858 non-null    object\n",
      " 5   Smokes (years)                      858 non-null    object\n",
      " 6   Smokes (packs/year)                 858 non-null    object\n",
      " 7   Hormonal Contraceptives             858 non-null    object\n",
      " 8   Hormonal Contraceptives (years)     858 non-null    object\n",
      " 9   IUD                                 858 non-null    object\n",
      " 10  IUD (years)                         858 non-null    object\n",
      " 11  STDs                                858 non-null    object\n",
      " 12  STDs (number)                       858 non-null    object\n",
      " 13  STDs:condylomatosis                 858 non-null    object\n",
      " 14  STDs:cervical condylomatosis        858 non-null    object\n",
      " 15  STDs:vaginal condylomatosis         858 non-null    object\n",
      " 16  STDs:vulvo-perineal condylomatosis  858 non-null    object\n",
      " 17  STDs:syphilis                       858 non-null    object\n",
      " 18  STDs:pelvic inflammatory disease    858 non-null    object\n",
      " 19  STDs:genital herpes                 858 non-null    object\n",
      " 20  STDs:molluscum contagiosum          858 non-null    object\n",
      " 21  STDs:AIDS                           858 non-null    object\n",
      " 22  STDs:HIV                            858 non-null    object\n",
      " 23  STDs:Hepatitis B                    858 non-null    object\n",
      " 24  STDs:HPV                            858 non-null    object\n",
      " 25  STDs: Number of diagnosis           858 non-null    int64 \n",
      " 26  STDs: Time since first diagnosis    858 non-null    object\n",
      " 27  STDs: Time since last diagnosis     858 non-null    object\n",
      " 28  Dx:Cancer                           858 non-null    int64 \n",
      " 29  Dx:CIN                              858 non-null    int64 \n",
      " 30  Dx:HPV                              858 non-null    int64 \n",
      " 31  Dx                                  858 non-null    int64 \n",
      " 32  Hinselmann                          858 non-null    int64 \n",
      " 33  Schiller                            858 non-null    int64 \n",
      " 34  Citology                            858 non-null    int64 \n",
      " 35  Biopsy                              858 non-null    int64 \n",
      "dtypes: int64(10), object(26)\n",
      "memory usage: 241.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                   0\n",
       "Number of sexual partners             0\n",
       "First sexual intercourse              0\n",
       "Num of pregnancies                    0\n",
       "Smokes                                0\n",
       "Smokes (years)                        0\n",
       "Smokes (packs/year)                   0\n",
       "Hormonal Contraceptives               0\n",
       "Hormonal Contraceptives (years)       0\n",
       "IUD                                   0\n",
       "IUD (years)                           0\n",
       "STDs                                  0\n",
       "STDs (number)                         0\n",
       "STDs:condylomatosis                   0\n",
       "STDs:cervical condylomatosis          0\n",
       "STDs:vaginal condylomatosis           0\n",
       "STDs:vulvo-perineal condylomatosis    0\n",
       "STDs:syphilis                         0\n",
       "STDs:pelvic inflammatory disease      0\n",
       "STDs:genital herpes                   0\n",
       "STDs:molluscum contagiosum            0\n",
       "STDs:AIDS                             0\n",
       "STDs:HIV                              0\n",
       "STDs:Hepatitis B                      0\n",
       "STDs:HPV                              0\n",
       "STDs: Number of diagnosis             0\n",
       "STDs: Time since first diagnosis      0\n",
       "STDs: Time since last diagnosis       0\n",
       "Dx:Cancer                             0\n",
       "Dx:CIN                                0\n",
       "Dx:HPV                                0\n",
       "Dx                                    0\n",
       "Hinselmann                            0\n",
       "Schiller                              0\n",
       "Citology                              0\n",
       "Biopsy                                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has 858 entries and 36 columns, consisting of datatypes int64 and object. There no null values detected in the dataframe.\n",
    "\n",
    "Out of 4 target columns - Hinselmann, Schiller, Cytology and Biopsy, the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 858 entries, 0 to 857\n",
      "Data columns (total 35 columns):\n",
      " #   Column                              Non-Null Count  Dtype \n",
      "---  ------                              --------------  ----- \n",
      " 0   Age                                 858 non-null    int64 \n",
      " 1   Number of sexual partners           858 non-null    object\n",
      " 2   First sexual intercourse            858 non-null    object\n",
      " 3   Num of pregnancies                  858 non-null    object\n",
      " 4   Smokes                              858 non-null    object\n",
      " 5   Smokes (years)                      858 non-null    object\n",
      " 6   Smokes (packs/year)                 858 non-null    object\n",
      " 7   Hormonal Contraceptives             858 non-null    object\n",
      " 8   Hormonal Contraceptives (years)     858 non-null    object\n",
      " 9   IUD                                 858 non-null    object\n",
      " 10  IUD (years)                         858 non-null    object\n",
      " 11  STDs                                858 non-null    object\n",
      " 12  STDs (number)                       858 non-null    object\n",
      " 13  STDs:condylomatosis                 858 non-null    object\n",
      " 14  STDs:cervical condylomatosis        858 non-null    object\n",
      " 15  STDs:vaginal condylomatosis         858 non-null    object\n",
      " 16  STDs:vulvo-perineal condylomatosis  858 non-null    object\n",
      " 17  STDs:syphilis                       858 non-null    object\n",
      " 18  STDs:pelvic inflammatory disease    858 non-null    object\n",
      " 19  STDs:genital herpes                 858 non-null    object\n",
      " 20  STDs:molluscum contagiosum          858 non-null    object\n",
      " 21  STDs:AIDS                           858 non-null    object\n",
      " 22  STDs:HIV                            858 non-null    object\n",
      " 23  STDs:Hepatitis B                    858 non-null    object\n",
      " 24  STDs:HPV                            858 non-null    object\n",
      " 25  STDs: Number of diagnosis           858 non-null    int64 \n",
      " 26  STDs: Time since first diagnosis    858 non-null    object\n",
      " 27  STDs: Time since last diagnosis     858 non-null    object\n",
      " 28  Dx:Cancer                           858 non-null    int64 \n",
      " 29  Dx:CIN                              858 non-null    int64 \n",
      " 30  Dx:HPV                              858 non-null    int64 \n",
      " 31  Dx                                  858 non-null    int64 \n",
      " 32  Hinselmann                          858 non-null    int64 \n",
      " 33  Schiller                            858 non-null    int64 \n",
      " 34  Citology                            858 non-null    int64 \n",
      "dtypes: int64(9), object(26)\n",
      "memory usage: 234.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Assigning features to X\n",
    "X = df.drop('Biopsy', axis = 1)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(858, 35)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ea0dc733fb28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assigning column 'Biopsy' as the target, y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Biopsy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "# Assigning column 'Biopsy' as the target, y\n",
    "y = df['Biopsy']\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "Split the dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into 80:10:10\n",
    "\n",
    "#splitting into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=0)\n",
    "\n",
    "#splitting training and validation set \n",
    "X_train, X_vald, y_train, y_vald = train_test_split(X, y, test_size = 0.1,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_vald.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_vald.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Perform data preprocessing such as normalization, standardization, label encoding etc.\n",
    "______________________________________________________________________________________\n",
    "Description:\n",
    "In this data preprocessing operation, we perform the following in order:\n",
    "1. Missing Value Detection and Replacement\n",
    "2. Outlier Detection\n",
    "3. Deal with Imbalanced Data\n",
    "4. Data Normalization\n",
    "5. Data Standardization\n",
    "\n",
    "We will perform the modelling on 2 sets of data later on, which are on the imbalanced data and balanced data to find out the relationship of balancing data with the types of machine learning model used and the performance of the model.\n",
    "\n",
    "At the end of this Data Preprocessing Section,\n",
    "\n",
    "Imbalanced data will be represented by:\n",
    "- X_train_std\n",
    "- X_vald_std\n",
    "- X_test_std\n",
    "- y_train\n",
    "- y_vald\n",
    "- y_test\n",
    "\n",
    "Balanced data will be represented by:\n",
    "- X_train_std_balanced\n",
    "- X_vald_std_balanced\n",
    "- X_test_std_balanced\n",
    "- y_train_balanced\n",
    "- y_vald_balanced\n",
    "- y_test_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Detection and Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the datatypes of each column\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the columns where datatype = 'object'\n",
    "X_train.select_dtypes(include='object').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that 26 columns have datatype 'object' although they consist mostly of floating point numbers. We can also see that some fields are filled with '?'. Thus, we can infer that the character '?' contributed to the heterogeneous data that resulted in the datatype 'object', and that '?' is used to represent missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in the columns\n",
    "X_train[X_train == '?'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' in the columns with nan\n",
    "X_train_nan = X_train.copy()\n",
    "X_train_nan.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Create an instance of SimpleImputer with mean as the strategy\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fill in the nan values with the mean of their column values\n",
    "X_train_imputed = pd.DataFrame(mean_imputer.fit_transform(X_train_nan), columns=X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "X_train_imputed.isna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any '?' remains in the columns\n",
    "X_train_imputed[X_train_imputed=='?'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns still consist of datatype = 'object'\n",
    "X_train_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the steps separately for validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation set\n",
    "\n",
    "# Replace '?' in the columns with nan\n",
    "X_vald_nan = X_vald.copy()\n",
    "X_vald_nan.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Fill in the nan values with the mean of their column values\n",
    "X_vald_imputed = pd.DataFrame(mean_imputer.fit_transform(X_vald_nan), columns=X_vald.columns)\n",
    "\n",
    "# Check for null values\n",
    "print(X_vald_imputed.isna().all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation Set\n",
    "\n",
    "# Check if any '?' remains in the columns\n",
    "print(X_vald_imputed[X_vald_imputed=='?'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test set\n",
    "\n",
    "# Replace '?' in the columns with nan\n",
    "X_test_nan = X_test.copy()\n",
    "X_test_nan.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Fill in the nan values with the mean of their column values\n",
    "X_test_imputed = pd.DataFrame(mean_imputer.fit_transform(X_test_nan), columns=X_vald.columns)\n",
    "\n",
    "# Check for null values\n",
    "X_test_imputed.isna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Set\n",
    "\n",
    "# Check if any '?' remains in the columns\n",
    "print(X_test_imputed[X_test_imputed=='?'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the threshold to determine the outliers\n",
    "\n",
    "def Outlier_Threshold(df, column):\n",
    "    \"\"\"\n",
    "    Calculates the outlier threshold for a given column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the data.\n",
    "    column (int): The index of the column for which to calculate the outlier threshold.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    The function calculates the outlier threshold using the Interquartile Range (IQR) method.\n",
    "    It determines the 75th percentile (Q3) and the 25th percentile (Q1) of the values in the column.\n",
    "    Then, it calculates the IQR by subtracting Q1 from Q3.\n",
    "    Finally, the function computes the outlier threshold as Q3 plus 1.5 times the IQR.\n",
    "    The result is printed for the specified column in the DataFrame.\n",
    "    \n",
    "    \"\"\"\n",
    "    for num in column:\n",
    "        Q3 = np.quantile(df.iloc[:,num:(num+1)],.75)\n",
    "        Q1 = np.quantile(df.iloc[:,num:(num+1)],.25)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_threshold = Q3+(1.5*IQR)\n",
    "        print(df.columns[num],\": \",outlier_threshold)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify the columns with outliers in a given dataframe\n",
    "\n",
    "def Outliers(df,outliers,std_dev_threshold, max_mean_ratio_threshold):\n",
    "    \"\"\"\n",
    "    Identifies the columns with outliers of a DataFrame based on specified thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the data.\n",
    "    outliers (list): A list to store the indices of the detected outliers.\n",
    "    std_dev_threshold (float): The threshold for standard deviation to determine outliers.\n",
    "    max_mean_ratio_threshold (float): The threshold for the maximum-mean ratio to determine outliers.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    The function iterates over the columns of the DataFrame and calculates the standard deviation (std_dev) and the maximum-mean ratio (max_mean_ratio) for each column.\n",
    "    If the std_dev of a column exceeds the specified std_dev_threshold or the max_mean_ratio exceeds the specified max_mean_ratio_threshold, the column index is appended to the outliers list.\n",
    "    The function prints the column name, its standard deviation, and the max-mean ratio if it is identified as an outlier.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_index = df.columns.get_loc(col)\n",
    "        std_dev = df[col].std()\n",
    "        if(df[col].mean()!=0 and df[col].max()!=1):\n",
    "            max_mean_ratio = float(df[col].max()/df[col].mean())\n",
    "        else:\n",
    "            continue\n",
    "        if (std_dev>std_dev_threshold or max_mean_ratio>max_mean_ratio_threshold):\n",
    "            outliers.append(col_index)\n",
    "            print(col,\"Standard Deviation: \",std_dev)\n",
    "            print(col,\"Max Mean Ratio: \",max_mean_ratio,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.iloc[:,0:18].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.iloc[:,18:].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can hypothesize that there are outliers present in some numerical columns by monitoring the standard deviation and difference between the max value and the mean value. The columns with outliers are identified by those whose standard deviation has a threshold of more than 3, or if the ratio of the max value over the mean value is more than 3 for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns with outliers \n",
    "# Stnadard deviation threshold = 3\n",
    "# Max Mean Ratio threshold = 3\n",
    "column_train = []\n",
    "Outliers(X_train_imputed, column_train, std_dev_threshold = 3, max_mean_ratio_threshold = 3)\n",
    "print(\"Columns with outliers (index): \",column_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified columns with outliers are:\n",
    "\n",
    "- Column[1]: Number of Sexual Partners\n",
    "- Column[3]: Num of pregnancies\n",
    "- Column[5]: Smokes (years)\n",
    "- Column[6]: Smokes (packs/year)\n",
    "- Column[8]: Hormonal Contraceptives (years)\n",
    "- Column[10]: IUD (years)\n",
    "- Column[12]: STDs (number)\n",
    "- Column[25]: STDs: Number of diagnosis\n",
    "- Column[26]: STDs: Time since first diagnosis\n",
    "- Column[27]: STDs: Time since last diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_outlier = X_train_imputed.copy()\n",
    "\n",
    "# Plot the distribution of the identified columns\n",
    "for num in column_train:\n",
    "    fig, (boxplot, hist) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Box plot and Histogram of '+ df.columns[num])\n",
    "    boxplot.boxplot(X_train_outlier.iloc[:,num:(num+1)])  # Boxplot\n",
    "    hist.hist(X_train_outlier.iloc[:,num:(num+1)])        # Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold for outliers for the identified columns\n",
    "Outlier_Threshold(X_train_outlier, column_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data visualizations above, we can observe that all the graphs are skewed to the right where outliers are present. This justifies the former method of outlier identification.\n",
    "\n",
    "The outlier threshold indicates the value where any field more than the threshold within that column is identified as an outlier. We can see that most of the columns identified have a threshold present. On the other hand, those columns of threshold 0 is due to the majority fields having the value '0', which resulted in a value of zero for both the first quartile and the third quartile.\n",
    "\n",
    "Therefore, we will be applying **Square Root transformation** to make the data more normally distributed and reduce the impact of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy of X_train_outlier where square root transformation will be applied\n",
    "X_train_sqrt = X_train_outlier.copy()   \n",
    "\n",
    "# Perform square root transformation on the columns with outliers\n",
    "X_train_sqrt.iloc[:,column_train] = np.sqrt(X_train_sqrt.iloc[:,column_train])\n",
    "X_train_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the transformed outlier columns\n",
    "for num in column_train:\n",
    "    fig, (boxplot, hist) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Box plot and Histogram of '+ df.columns[num])\n",
    "    boxplot.boxplot(X_train_sqrt.iloc[:,num:(num+1)])  # Boxplot\n",
    "    hist.hist(X_train_sqrt.iloc[:,num:(num+1)])        # Histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_imputed.iloc[:,0:18].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_imputed.iloc[:,18:].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same methods to identify the columns with outliers in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns with outliers \n",
    "# Stnadard deviation threshold = 3\n",
    "# Max Mean Ratio threshold = 3\n",
    "column_vald = []\n",
    "Outliers(X_vald_imputed, column_vald, std_dev_threshold = 3, max_mean_ratio_threshold = 3)\n",
    "print(\"Columns with outliers (index): \",column_vald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified columns with outliers are:\n",
    "- Column[0]: Age\n",
    "- Column[3]: Num of pregnancies\n",
    "- Column[5]: Smokes (years)\n",
    "- Column[6]: Smokes (packs/year)\n",
    "- Column[8]: Hormonal Contraceptives (years)\n",
    "- Column[10]: IUD (years)\n",
    "- Column[12]: STDs (number)\n",
    "\n",
    "From the plots below, we can see that the distributions of the columns stated above are skewed to the right, and outliers are indeed present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_outlier = X_vald_imputed.copy()\n",
    "\n",
    "# Plot the distribution of the outlier columns\n",
    "for num in column_vald:\n",
    "    fig, (boxplot, hist) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Box plot and Histogram of '+ df.columns[num])\n",
    "    boxplot.boxplot(X_vald_outlier.iloc[:,num:(num+1)])  # Boxplot\n",
    "    hist.hist(X_vald_outlier.iloc[:,num:(num+1)])        # Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold for outliers for the identified columns\n",
    "Outlier_Threshold(X_train_outlier, column_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy of X_vald_sqrt where square root transformation will be applied\n",
    "X_vald_sqrt = X_vald_outlier.copy()   \n",
    "\n",
    "# Perform square root transformation on the columns with outliers\n",
    "X_vald_sqrt.iloc[:,column_vald] = np.sqrt(X_vald_sqrt.iloc[:,column_vald])\n",
    "X_vald_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the transformed outlier columns\n",
    "for num in column_vald:\n",
    "    fig, (boxplot, hist) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Box plot and Histogram of '+ df.columns[num])\n",
    "    boxplot.boxplot(X_vald_sqrt.iloc[:,num:(num+1)])  # Boxplot\n",
    "    hist.hist(X_vald_sqrt.iloc[:,num:(num+1)])        # Histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed.iloc[:,0:18].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed.iloc[:,18:].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same methods to identify the columns with outliers in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns with outliers \n",
    "# Stnadard deviation threshold = 3\n",
    "# Max Mean Ratio threshold = 3\n",
    "column_test = []\n",
    "Outliers(X_test_imputed, column_test, std_dev_threshold = 3, max_mean_ratio_threshold = 3)\n",
    "print(\"Columns with outliers (index): \",column_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified columns with outliers are:\n",
    "- Column[0]: Age\n",
    "- Column[3]: Num of pregnancies\n",
    "- Column[5]: Smokes (years)\n",
    "- Column[6]: Smokes (packs/year)\n",
    "- Column[8]: Hormonal Contraceptives (years)\n",
    "- Column[10]: IUD (years)\n",
    "- Column[12]: STDs (number)\n",
    "\n",
    "From the plots below, we can see that the distributions of the columns stated above are skewed to the right, and outliers are indeed present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_outlier = X_test_imputed.copy()\n",
    "\n",
    "# Plot the distribution of the outlier columns\n",
    "for num in column_test:\n",
    "    fig, (boxplot, hist) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Box plot and Histogram of '+ df.columns[num])\n",
    "    boxplot.boxplot(X_test_outlier.iloc[:,num:(num+1)])  # Boxplot\n",
    "    hist.hist(X_test_outlier.iloc[:,num:(num+1)])        # Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy of X_vald_sqrt where square root transformation will be applied\n",
    "X_test_sqrt = X_test_outlier.copy()   \n",
    "\n",
    "# Perform square root transformation on the columns with outliers\n",
    "X_test_sqrt.iloc[:,column_test] = np.sqrt(X_test_sqrt.iloc[:,column_test])\n",
    "X_test_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the transformed outlier columns\n",
    "for num in column_test:\n",
    "    fig, (boxplot, hist) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Box plot and Histogram of '+ df.columns[num])\n",
    "    boxplot.boxplot(X_test_sqrt.iloc[:,num:(num+1)])  # Boxplot\n",
    "    hist.hist(X_test_sqrt.iloc[:,num:(num+1)])        # Histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of values for each class in y\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph to visualize the imbalance of the target column\n",
    "y_train_countplot = sns.countplot(y_train)\n",
    "\n",
    "for container in y_train_countplot.containers:\n",
    "    y_train_countplot.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe from the numbers and graph above, we can see that there is an extreme uneven distribution of values for class 0 and 1. The majority class is 94%（722 out of 772 records) with a negative biopsy diagnosis, while the minority class is only 6% (50 out of 772 records) with a positive biopsy diagnosis. This may cause a biased prediction by our classification models.\n",
    "\n",
    "Thus, we will deal with this imbalance by oversampling the minority class by resampling, using the resample() function from the sklearn.utils library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for y_train to concatenate with X_train_sqrt\n",
    "y_train_reset_index = (y_train.reset_index()).drop('index', axis = 1)\n",
    "y_train_reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining X_train and y_train into a dataframe for easy operation\n",
    "train = pd.concat([X_train_sqrt, y_train_reset_index], axis = 1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two different dataframe for class 0 and 1\n",
    "train_major = train[(train['Biopsy']==0)]  # class 0 is the majority class\n",
    "train_minor = train[(train['Biopsy']==1)]  # class 1 is the minority class\n",
    "\n",
    "# Oversample the minority class with the number of majority class = 722\n",
    "train_minor_upsampled = resample(train_minor, replace=True, n_samples = 722, random_state = 42)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "train_upsampled = pd.concat([train_minor_upsampled, train_major])\n",
    "\n",
    "# Shuffle the rows in the dataframe, so that the class samples are not grouped together\n",
    "train_upsampled = shuffle(train_upsampled)\n",
    "\n",
    "train_upsampled['Biopsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the value for each class is balanced\n",
    "train_upsampled['Biopsy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the balanced data\n",
    "y_train_countplot = sns.countplot(data = train_upsampled, x = 'Biopsy')\n",
    "\n",
    "for container in y_train_countplot.containers:\n",
    "    y_train_countplot.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the class distribution is now balanced, with 722 records respectively for both class 0 and class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe back into feature columns and target column\n",
    "X_train_balanced = train_upsampled.drop('Biopsy', axis = 1)\n",
    "y_train_balanced = train_upsampled['Biopsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the steps separately for validation set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of values for each class in y\n",
    "y_vald.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for y_vald to concatenate with X_vald_sqrt\n",
    "y_vald_reset_index = (y_vald.reset_index()).drop('index', axis = 1)\n",
    "\n",
    "# Combining X_train and y_train into a dataframe for easy operation\n",
    "vald = pd.concat([X_vald_sqrt, y_vald_reset_index], axis = 1)\n",
    "\n",
    "# Create two different dataframe for class 0 and 1\n",
    "vald_major = vald[(vald['Biopsy']==0)]  # class 0 is the majority class\n",
    "vald_minor = vald[(vald['Biopsy']==1)]  # class 1 is the minority class\n",
    "\n",
    "# Oversample the minority class with the number of majority class = 81\n",
    "vald_minor_upsampled = resample(vald_minor, replace=True, n_samples = 81, random_state = 42)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "vald_upsampled = pd.concat([vald_minor_upsampled, vald_major])\n",
    "\n",
    "# Shuffle the rows in the dataframe, so that the class samples are not grouped together\n",
    "vald_upsampled = shuffle(vald_upsampled)\n",
    "\n",
    "vald_upsampled['Biopsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the value for each class is balanced\n",
    "vald_upsampled['Biopsy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the balanced data\n",
    "y_vald_countplot = sns.countplot(data = vald_upsampled, x = \"Biopsy\")\n",
    "\n",
    "for container in y_vald_countplot.containers:\n",
    "    y_vald_countplot.bar_label(container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vald_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe back into feature columns and target column\n",
    "X_vald_balanced = vald_upsampled.drop('Biopsy', axis = 1)\n",
    "y_vald_balanced = vald_upsampled['Biopsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vald_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of values for each class in y\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for y_test to concatenate with X_test_sqrt\n",
    "y_test_reset_index = (y_test.reset_index()).drop('index', axis = 1)\n",
    "\n",
    "# Combining X_train and y_train into a dataframe for easy operation\n",
    "test = pd.concat([X_test_sqrt, y_test_reset_index], axis = 1)\n",
    "\n",
    "# Create two different dataframe for class 0 and 1\n",
    "test_major = test[(test['Biopsy']==0)]  # class 0 is the majority class\n",
    "test_minor = test[(test['Biopsy']==1)]  # class 1 is the minority class\n",
    "\n",
    "# Oversample the minority class with the number of majority class = 81\n",
    "test_minor_upsampled = resample(test_minor, replace=True, n_samples = 81, random_state = 42)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "test_upsampled = pd.concat([test_minor_upsampled, test_major])\n",
    "\n",
    "# Shuffle the rows in the dataframe, so that the class samples are not grouped together\n",
    "test_upsampled = shuffle(test_upsampled)\n",
    "\n",
    "test_upsampled['Biopsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the value for each class is balanced\n",
    "test_upsampled['Biopsy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the balanced data\n",
    "y_test_countplot = sns.countplot(data = test_upsampled, x = \"Biopsy\")\n",
    "\n",
    "for container in y_test_countplot.containers:\n",
    "    y_test_countplot.bar_label(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe back into feature columns and target column\n",
    "X_test_balanced = test_upsampled.drop('Biopsy', axis = 1)\n",
    "y_test_balanced = test_upsampled['Biopsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_need_norm = [] # Columns in the dataframe that needs to be normalized\n",
    "\n",
    "# Function for Normalisation using MinMaxScaler\n",
    "def Norm_MinMaxScaler(df):\n",
    "    for num in columns_need_norm:\n",
    "        df_copy = df\n",
    "        scaler = MinMaxScaler()\n",
    "        df_copy.iloc[:,num:(num+1)] = scaler.fit_transform(df_copy.iloc[:,num:(num+1)])\n",
    "    df_normalized = pd.DataFrame(df_copy, columns = df.columns)\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sqrt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the max values of each column, we can see that the following columns from both balanced data and imbalanced data needs to be normalised:\n",
    "\n",
    "- Column[0]: Age\n",
    "- Column[1]: Number of sexual partners\n",
    "- Column[2]: First sexual intercourse\n",
    "- Column[3]: Num of pregnancies\n",
    "- Column[5]: Smokes (years)\n",
    "- Column[6]: Smokes (packs/year)\n",
    "- Column[8]: Hormonal Contraceptives (years)\n",
    "- Column[10]: IUD (years)\n",
    "- Column[12]: STDs (number)\n",
    "- Column[25]: STDs: Number of diagnosis\n",
    "- Column[26]: STDs: Time since first diagnosis\n",
    "- Column[27]: STDs: Time since last diagnosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns identified that needs normalization\n",
    "columns_need_norm = [0,1,2,3,5,6,8,10,12,25,26,27]\n",
    "\n",
    "X_train_norm = Norm_MinMaxScaler(X_train_sqrt)\n",
    "X_vald_norm = Norm_MinMaxScaler(X_vald_sqrt)\n",
    "X_test_norm = Norm_MinMaxScaler(X_test_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns identified that needs normalization\n",
    "columns_need_norm = [0,1,2,3,5,6,8,10,12,25,26,27]\n",
    "\n",
    "X_train_norm_balanced = Norm_MinMaxScaler(X_train_balanced)\n",
    "X_vald_norm_balanced = Norm_MinMaxScaler(X_vald_balanced)\n",
    "X_test_norm_balanced = Norm_MinMaxScaler(X_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_norm_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_need_std = [] # Columns in the dataframe that needs to be standardized\n",
    "\n",
    "# Function for Normalisation using MinMaxScaler\n",
    "def Std_StandardScaler(df):\n",
    "    for num in columns_need_norm:\n",
    "        df_copy = df\n",
    "        scaler = StandardScaler()\n",
    "        df_copy.iloc[:,num:(num+1)] = scaler.fit_transform(df_copy.iloc[:,num:(num+1)])\n",
    "    df_normalized = pd.DataFrame(df_copy, columns = df.columns)\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to standardize only numeric data and not categorical data, therefore only the following columns are chosen, similar to those that are normalized:\n",
    "\n",
    "- Column[0]: Age\n",
    "- Column[1]: Number of sexual partners\n",
    "- Column[2]: First sexual intercourse\n",
    "- Column[3]: Num of pregnancies\n",
    "- Column[5]: Smokes (years)\n",
    "- Column[6]: Smokes (packs/year)\n",
    "- Column[8]: Hormonal Contraceptives (years)\n",
    "- Column[10]: IUD (years)\n",
    "- Column[12]: STDs (number)\n",
    "- Column[25]: STDs: Number of diagnosis\n",
    "- Column[26]: STDs: Time since first diagnosis\n",
    "- Column[27]: STDs: Time since last diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns identified that needs normalization\n",
    "columns_need_std = [0,1,2,3,5,6,8,10,12,25,26,27]\n",
    "\n",
    "X_train_std = Std_StandardScaler(X_train_norm)\n",
    "X_vald_std = Std_StandardScaler(X_vald_norm)\n",
    "X_test_std = Std_StandardScaler(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns identified that needs normalization\n",
    "columns_need_std = [0,1,2,3,5,6,8,10,12,25,26,27]\n",
    "\n",
    "X_train_std_balanced = Std_StandardScaler(X_train_norm_balanced)\n",
    "X_vald_std_balanced = Std_StandardScaler(X_vald_norm_balanced)\n",
    "X_test_std_balanced = Std_StandardScaler(X_test_norm_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_std_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Perform feature selection to select the relevant features.\n",
    "______________________________________________________________________________________\n",
    "Description:\n",
    "\n",
    "Feature selection is done to select the features that most correlated with our target feature. \n",
    "So that we can :\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "\n",
    "- Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "\n",
    "- Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.\n",
    "\n",
    "We will be conducting the feaature selection by \n",
    "1. Building a Correlation Matrix Heatmap (using Pearson Correlation Coefficient)\n",
    "2. Identifying the features with correlation coefficient closer to 0 (weaker correlation)\n",
    "3. Drop the columns with weak correlation\n",
    "\n",
    "At the end of this Feature Selection Section,\n",
    "\n",
    "Imbalanced data will be represented by:\n",
    "- X_train\n",
    "- X_vald\n",
    "- X_test\n",
    "- y_train\n",
    "- y_vald\n",
    "- y_test\n",
    "\n",
    "Balanced data will be represented by:\n",
    "- X_train_balanced\n",
    "- X_vald_balanced\n",
    "- X_test_balanced\n",
    "- y_train_balanced\n",
    "- y_vald_balanced\n",
    "- y_test_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Correlation Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X_train_std and y_train\n",
    "train_corr = pd.concat([X_train_std, y_train_reset_index], axis = 1)\n",
    "train_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix on the imbalanced training set\n",
    "corr_matrix = train_corr.corr(method = 'pearson')\n",
    "top_corr_features = corr_matrix.index\n",
    "\n",
    "# Plot the correlation matrix heat map\n",
    "plt.figure(figsize=(30,30))\n",
    "g=sns.heatmap(train_corr[top_corr_features].corr(),annot=True,cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Identifying the low correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out the correlation coefficients of the features and the target value by ascending order\n",
    "corr_biopsy = pd.DataFrame(corr_matrix['Biopsy'])\n",
    "corr_biopsy.sort_values(\"Biopsy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify that the columns 'STDs:cervical condylomatosis' and 'STDs:AIDS' in this dataframe. On the other hand, columns 'STDs:HPV', 'STDs:molluscum contagiosum', 'STDs:pelvic inflammatory disease', 'STDs:Hepatitis B', 'Hormonal Contraceptives' and 'First sexual intercourse' all have a low magnitude of correlation coefficient regardless of the direction, which are all not more than 0.02.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = ['STDs:cervical condylomatosis','STDs:AIDS','STDs:HPV','STDs:molluscum contagiosum','STDs:pelvic inflammatory disease','STDs:Hepatitis B','Hormonal Contraceptives','First sexual intercourse']\n",
    "drop_features_index = [df.columns.get_loc(col)for col in drop_features]\n",
    "drop_features_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the identified columns to be dropped are:\n",
    "- Column[2]: First sexual intercourse\n",
    "- Column[7]: Hormonal Contraceptives\n",
    "- Column[14]: STDs:cervical condylomatosis\n",
    "- Column[18]: STDs:pelvic inflammatory disease\n",
    "- Column[20]: STDs:molluscum contagiosum\n",
    "- Column[21]: STDs:AIDS\n",
    "- Column[23]: STDs:Hepatitis B\n",
    "- Column[24]: STDs:HPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Drop low correlating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the identify columns in the training, validation and test set of the imbalanced data\n",
    "for string in drop_features:\n",
    "    X_train = X_train_std.drop(axis = 1, columns = drop_features)\n",
    "    X_vald = X_vald_std.drop(axis = 1, columns = drop_features)\n",
    "    X_test = X_test_std.drop(axis = 1, columns = drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for training, validation and test set of the balanced data\n",
    "for string in drop_features:\n",
    "    X_train_balanced = X_train_std_balanced.drop(axis = 1, columns = drop_features)\n",
    "    X_vald_balanced = X_vald_std_balanced.drop(axis = 1, columns = drop_features)\n",
    "    X_test_balanced = X_test_std_balanced.drop(axis = 1, columns = drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vald_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modeling\n",
    "Build the machine learning models. You must build atleast two (2) predictive models. One of the predictive models must be either Decision Tree or Support Vector Machine.\n",
    "______________________________________________________________________________________\n",
    "Description: Our group selected Decision Tree and Support Vector Machine(SVM), we built 2 predictive model to predict the target variable of cervical cancer dataset which is 'biopsy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "#Build the model \n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on validation set\n",
    "y_vald_pred= tree_model.predict(X_vald_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the performance\n",
    "accuracy = accuracy_score(y_vald_balanced, y_vald_pred)\n",
    "precision = precision_score(y_vald_balanced, y_vald_pred)\n",
    "recall = recall_score(y_vald_balanced, y_vald_pred)\n",
    "f1 = f1_score(y_vald_balanced, y_vald_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "conf_m =confusion_matrix (y_vald_balanced,y_vald_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize confusion matrix\n",
    "sns.heatmap(conf_m, annot = True, fmt='d',cmap='BuPu')\n",
    "plt.title('Confusion Matrix of decision tree')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print accuracy,precision,recall and f1\n",
    "print(\"Accuracy of decision tree:\", accuracy)\n",
    "print(\"Precision of decision tree:\", precision)\n",
    "print(\"Recall of decision tree:\", recall)\n",
    "print(\"F1 Score of decision tree:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_test_pred= tree_model.predict(X_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate decision tree model's performance on test set\n",
    "test_accuracy = accuracy_score(y_test_balanced, y_test_pred)\n",
    "print(\"Test Accuracy of decision tree:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "The second predictive model we have chose is Support Vector Machine (SVM). SVM algorithms determine the hyperplane boundary to classify data and make predictions. The three popular types of kernels used in SVM are:\n",
    "\n",
    "<li> Linear Kernel\n",
    "<li> Polynomial Kernel\n",
    "<li> Radial Basis Function (RBF) Kernel\n",
    "<li> Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import svm model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine which kernel is the most suitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for hyperparameter tuning\n",
    "para_linear = {'C': [0.1, 1, 10]}\n",
    "para_poly = {'C': [0.1, 1, 10], 'degree': [2, 3, 4]}\n",
    "para_RBF = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "para_sigmoid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning using grid search\n",
    "clf_linear = GridSearchCV(SVC(kernel='linear'), para_linear, cv=5)\n",
    "clf_poly = GridSearchCV(SVC(kernel='poly'), para_poly, cv=5)\n",
    "clf_RBF = GridSearchCV(SVC(kernel='rbf'), para_RBF, cv=5)\n",
    "clf_sigmoid = GridSearchCV(SVC(kernel='sigmoid'), para_sigmoid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using unbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Kernel Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with linear kernel and find the best model\n",
    "clf_linear.fit(X_train, y_train)\n",
    "best_linear = clf_linear.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y_pred_linear_vald = best_linear.predict(X_vald)\n",
    "y_pred_linear_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_pred_linear = best_linear.predict(X_test)\n",
    "y_pred_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial Kernel Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with polynomial kernel and find the best model\n",
    "clf_poly.fit(X_train, y_train)\n",
    "best_poly = clf_poly.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y_pred_poly_vald = best_poly.predict(X_vald)\n",
    "y_pred_poly_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_pred_poly = best_poly.predict(X_test)\n",
    "y_pred_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RBF Kernel Model Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with RBF kernel and find the best model\n",
    "clf_RBF.fit(X_train, y_train)\n",
    "best_RBF = clf_RBF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y_pred_RBF_vald = best_RBF.predict(X_vald)\n",
    "y_pred_RBF_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_pred_RBF = best_RBF.predict(X_test)\n",
    "y_pred_RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Kernel Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with sigmoid kernel and find the best model\n",
    "clf_sigmoid.fit(X_train, y_train)\n",
    "best_sigmoid = clf_sigmoid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y_pred_sigmoid_vald = best_sigmoid.predict(X_vald)\n",
    "y_pred_sigmoid_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y_pred_sigmoid = best_sigmoid.predict(X_test)\n",
    "y_pred_sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on the validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for linear kernel:\",metrics.accuracy_score(y_vald, y_pred_linear_vald))\n",
    "print(\"Accuracy for polynomial kernel:\",metrics.accuracy_score(y_vald, y_pred_poly_vald))\n",
    "print(\"Accuracy for RBF kernel:\",metrics.accuracy_score(y_vald, y_pred_RBF_vald))\n",
    "print(\"Accuracy for sigmoid kernel:\",metrics.accuracy_score(y_vald, y_pred_sigmoid_vald))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for linear kernel:\",metrics.accuracy_score(y_test, y_pred_linear))\n",
    "print(\"Accuracy for polynomial kernel:\",metrics.accuracy_score(y_test, y_pred_poly))\n",
    "print(\"Accuracy for RBF kernel:\",metrics.accuracy_score(y_test, y_pred_RBF))\n",
    "print(\"Accuracy for sigmoid kernel:\",metrics.accuracy_score(y_test, y_pred_sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using the balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Kernel Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with linear kernel and find the best model\n",
    "clf_linear.fit(X_train_balanced, y_train_balanced)\n",
    "best_linear1 = clf_linear.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y1_pred_linear_vald = best_linear1.predict(X_vald_balanced)\n",
    "y1_pred_linear_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y1_pred_linear = best_linear1.predict(X_test_balanced)\n",
    "y1_pred_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial Kernel Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with polynomial kernel and find the best model\n",
    "clf_poly.fit(X_train_balanced, y_train_balanced)\n",
    "best_poly1 = clf_poly.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y1_pred_poly_vald = best_poly1.predict(X_vald_balanced)\n",
    "y1_pred_poly_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y1_pred_poly = best_poly1.predict(X_test_balanced)\n",
    "y1_pred_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RBF Kernel Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with RBF kernel and find the best model\n",
    "clf_RBF.fit(X_train_balanced, y_train_balanced)\n",
    "best_RBF1 = clf_RBF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y1_pred_RBF_vald = best_RBF1.predict(X_vald_balanced)\n",
    "y1_pred_RBF_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y1_pred_RBF = best_RBF1.predict(X_test_balanced)\n",
    "y1_pred_RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Kernel Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with sigmoid kernel and find the best model\n",
    "clf_sigmoid.fit(X_train_balanced, y_train_balanced)\n",
    "best_sigmoid1 = clf_sigmoid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the validation set\n",
    "y1_pred_sigmoid_vald = best_sigmoid1.predict(X_vald_balanced)\n",
    "y1_pred_sigmoid_vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "y1_pred_sigmoid = best_sigmoid1.predict(X_test_balanced)\n",
    "y1_pred_sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on the validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for linear kernel:\",metrics.accuracy_score(y_vald_balanced, y1_pred_linear_vald))\n",
    "print(\"Accuracy for polynomial kernel:\",metrics.accuracy_score(y_vald_balanced, y1_pred_poly_vald))\n",
    "print(\"Accuracy for RBF kernel:\",metrics.accuracy_score(y_vald_balanced, y1_pred_RBF_vald))\n",
    "print(\"Accuracy for sigmoid kernel:\",metrics.accuracy_score(y_vald_balanced, y1_pred_sigmoid_vald))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for linear kernel:\",metrics.accuracy_score(y_test_balanced, y1_pred_linear))\n",
    "print(\"Accuracy for polynomial kernel:\",metrics.accuracy_score(y_test_balanced, y1_pred_poly))\n",
    "print(\"Accuracy for RBF kernel:\",metrics.accuracy_score(y_test_balanced, y1_pred_RBF))\n",
    "print(\"Accuracy for sigmoid kernel:\",metrics.accuracy_score(y_test_balanced, y1_pred_sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models\n",
    "Perform a comparison between the predictive models. <br>\n",
    "Report the accuracy, recall, precision and F1-score measures as well as the confusion matrix if it is a classification problem. <br>\n",
    "Report the R2 score, mean squared error and mean absolute error if it is a regression problem.\n",
    "______________________________________________________________________________________\n",
    "Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
